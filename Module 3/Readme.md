# ğŸ¤– Machine Learning with Python â€“ Module 3 (Classification & Decision Trees)

[![Made with Python](https://img.shields.io/badge/Made%20with-Python-blue?logo=python)](https://www.python.org/)
[![Course: IBM](https://img.shields.io/badge/Course-IBM%20Machine%20Learning%20with%20Python-lightblue?logo=ibm)](https://www.coursera.org/learn/machine-learning-with-python/home/module/3)
[![Tool: Scikit-learn](https://img.shields.io/badge/Library-Scikit--learn-orange?logo=scikitlearn)](https://scikit-learn.org/)
[![Tool: XGBoost](https://img.shields.io/badge/Library-XGBoost-green?logo=xgboost)](https://xgboost.readthedocs.io/)
[![Status: Completed](https://img.shields.io/badge/Status-Completed-success?style=flat)](https://www.coursera.org/learn/machine-learning-with-python)

> ğŸ“˜ Completed hands-on module from **IBMâ€™s Machine Learning with Python (Coursera)** â€” exploring **classification, decision trees, SVM, KNN, regression trees, and ensemble models** with real-world datasets.

---

## ğŸ§  Overview

This repository contains my **projects** and **comprehensive notes** from **Module 3: Classification & Decision Trees** of the [Machine Learning with Python](https://www.coursera.org/learn/machine-learning-with-python) course by IBM.  
In this module, I implemented multiple **supervised learning algorithms** and evaluated their performance using metrics like accuracy, MSE, and ROC-AUC.

---

## ğŸ¯ Learning Objectives

- Understand how **classification models** predict categorical outcomes  
- Implement **Decision Trees** for classification and regression tasks  
- Apply **Support Vector Machines (SVM)** for fraud detection  
- Use **K-Nearest Neighbors (KNN)** for customer segmentation  
- Build and evaluate **Regression Trees** for continuous prediction  
- Train and compare **Random Forest** and **XGBoost** ensemble models  
- Explore **biasâ€“variance tradeoff** and learn how ensemble models mitigate it  

---

## ğŸ§© Projects Completed

| ğŸ§¾ Project Name | ğŸ§® Algorithm(s) Used | ğŸ“Š Description |
|-----------------|----------------------|----------------|
| `decision_tree_svm_ccFraud.ipynb` | Decision Tree, SVM | Credit card fraud detection using binary classification |
| `Decision_trees.ipynb` | Decision Tree Classifier | Drug prescription classification using patient data |
| `Regression_Trees_Taxi_Tip.ipynb` | Decision Tree Regressor | Predicting taxi trip tips from NYC dataset |
| `KNN_Classification.ipynb` | K-Nearest Neighbors | Classifying telecom customers into service categories |
| `Multi-class_Classification.ipynb` | Logistic Regression (OvA, OvO) | Obesity risk prediction with multi-class classification |
| `Random_Forests_XGBoost.ipynb` | Random Forest, XGBoost | Comparing ensemble regression models on housing data |

---

## ğŸ“‚ Repository Contents

| File | Description |
|------|--------------|
| `Machine_Learning_Notes_M3.pdf` | My structured notes covering all supervised ML models from Module 3. |
| `decision_tree_svm_ccFraud.ipynb` | Fraud detection using Decision Trees and SVMs. |
| `Decision_trees.ipynb` | Drug type prediction with Decision Tree classifier. |
| `KNN_Classification.ipynb` | KNN applied on telecom dataset. |
| `Multi-class_Classification.ipynb` | Logistic regression for multi-class classification. |
| `Regression_Trees_Taxi_Tip.ipynb` | Taxi tip prediction using Regression Trees. |
| `Random_Forests_XGBoost.ipynb` | Random Forest vs XGBoost regression performance comparison. |

---

## âš™ï¸ Tools & Libraries

- **Programming:** Python ğŸ  
- **Libraries:**  
  `Scikit-learn`, `NumPy`, `Pandas`, `Matplotlib`, `Seaborn`, `XGBoost`  
- **Environment:** Jupyter Notebook  
- **Datasets:**  
  - Credit Card Fraud (Kaggle)  
  - NYC Taxi Trip Data  
  - California Housing Dataset  
  - Telecom Customer Dataset  
  - UCI Obesity Risk Dataset  
  - Drug Classification Dataset  

---

## ğŸ“Š Key Learnings & Takeaways

- **Classification** is a core supervised learning task predicting discrete labels.  
- **Decision Trees** are interpretable and powerful for both regression and classification.  
- **KNN** is simple yet sensitive to feature scaling and data size.  
- **SVMs** perform well on high-dimensional, small-to-medium datasets.  
- **Regression Trees** predict continuous outcomes using hierarchical splits.  
- **Random Forest** (bagging) and **XGBoost** (boosting) significantly improve performance.  
- **Biasâ€“Variance Tradeoff:** Finding the sweet spot between underfitting and overfitting.  

---

## ğŸŒ² Module Highlights

- Built **six supervised ML models** from scratch using Scikit-learn and XGBoost.  
- Applied **classification and regression techniques** on real datasets.  
- Compared model accuracy, ROC-AUC, and error metrics.  
- Created a **Supervised ML Cheat Sheet** for quick revision and reference.  

---

## ğŸ‘¨â€ğŸ’» Author

**Sandeep Maurya**  
ğŸ“ Aspiring Data Scientist & ML Engineer  
ğŸ“§ [isandeeep06@gmail.com](mailto:isandeeep06@gmail.com)  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/sandeepmaurya-datascientist/)

---

## ğŸ§© Acknowledgement

- **Course:** [Machine Learning with Python â€“ IBM (Coursera)](https://www.coursera.org/learn/machine-learning-with-python)  
- **Instructor:** IBM Skills Network  

---

## ğŸŒŸ Support

If you found these notes or projects helpful:  
â­ **Star this repo** â†’ it motivates me to share more learning resources  
ğŸ“¢ **Share it** with fellow learners  
ğŸ’¬ **Feedback or suggestions?** Open an issue!

---

> _â€œBagging reduces variance, boosting reduces bias â€” and understanding both makes you a machine learning engineer.â€_ ğŸ’¡

